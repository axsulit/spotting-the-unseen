import os
import argparse
import yaml
import torch
from pathlib import Path
from tqdm import tqdm
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

from model.network.Recce import Recce
from dataset import get_dataloader

def parse_args():
    parser = argparse.ArgumentParser(description='Test RECCE model')
    parser.add_argument('--config', type=str, required=True, help='Path to config file')
    parser.add_argument('--checkpoint', type=str, required=True, help='Path to model checkpoint (best_model.pt or latest_model.pt)')
    parser.add_argument('--output_dir', type=str, default='test_results', help='Directory to save test results')
    return parser.parse_args()

def plot_confusion_matrix(cm, output_path):
    """Plot and save confusion matrix."""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig(output_path)
    plt.close()

def main():
    args = parse_args()
    
    # Load configuration
    with open(args.config) as f:
        config = yaml.safe_load(f)
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Set device
    device = torch.device(config["train"]["device"])
    print(f"Using device: {device}")
    
    # Initialize model
    model = Recce(num_classes=config["model"]["num_classes"]).to(device)
    
    # Load checkpoint
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    print(f"Loaded checkpoint from epoch {checkpoint['epoch']}")
    
    # Create test dataloader
    test_loader = get_dataloader("test")
    
    # Test model
    model.eval()
    all_predictions = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Testing"):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)

            if outputs.dim() == 0:
                outputs = outputs.unsqueeze(0)  # Add batch dimension
            elif outputs.dim() == 1:
                outputs = outputs.unsqueeze(0)  # Add batch dimension if missing
            
            outputs = outputs.squeeze()
            probs = torch.sigmoid(outputs)
            predictions = (probs > 0.5).long()
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    # Convert to numpy arrays
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)
    
    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_predictions)
    precision = precision_score(all_labels, all_predictions)
    recall = recall_score(all_labels, all_predictions)
    f1 = f1_score(all_labels, all_predictions)
    auc = roc_auc_score(all_labels, all_probs)
    cm = confusion_matrix(all_labels, all_predictions)
    
    # Print results
    print("\nTest Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {auc:.4f}")
    print("\nConfusion Matrix:")
    print(cm)
    
    # Save results
    results = {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "auc": auc,
        "confusion_matrix": cm.tolist()
    }
    
    with open(output_dir / "test_results.yml", "w") as f:
        yaml.dump(results, f)
    
    # Plot and save confusion matrix
    plot_confusion_matrix(cm, output_dir / "confusion_matrix.png")
    
    print(f"\nResults saved to {output_dir}")

if __name__ == "__main__":
    main()
